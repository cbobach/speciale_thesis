In this chapter I will introduce the different shuffling algorithms studied during this project. I will introduce the ideas behind each algorithm studied and what makes it special. I will introduce the idea behind why these were chosen. I will explain how they were optimized to fit better to the specific needs for an application as a poker game. Lastly I will compare the algorithms to see the different benefits. 

All the permutation algorithms studied are with the purpose of shuffling card decks. It is important to chose an algorithm that ensures that the correct amount of permutations is reached.

The first algorithm studied is the Fisher-Yates algorithm introduced in \citeB{fisher_yates}. It may also be known as Knuth shuffle and was introduced to computer science by R. Durstenfeld in \citeA{durstenfeld} as algorithm 235. This algorithm uses an in place permutation approach and gives a perfect uniform random permutation.

The second algorithm proposed uses ideas from shuffling networks which is introduced in \citeA{psi} and the well known $bubble\text{-}sort$ algorithm. The idea is simple and use conditional swaps which swaps two inputs based on some condition. Tjis also yields a perfect uniform permutation.
An other type of shuffling networks called Bitonic shuffle network will be introduced and discussed. But no implementation of such a shuffle network was done.

All these shuffle algorithms is optimized to fit to the poker game introduced in the section on poker in chapter \ref{ch:intro}. This is done such that it only shuffles the first cards that are used during a game and not the whole deck.

%%%%%%%%%%%%%%%%%%%%%%% FISHER YATES %%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Fisher-Yates}
\input{fisher_yates}
$Fisher\text{-}Yates$ is a well known in place permutation algorithm that given two arrays as input one of some values that should be shuffled and another such that the first array is shuffle accordingly to the values of the second array. These swap values of the second array indicate where each of the original values should go in the swapp. When the algorithm runs through the first array which is supposed to be permuted it swaps the value at an given index whit the value specified by the swap value of the second array. Think of the input to be shuffled as a card deck then you take the top card of the deck and swap it with another card at a position predefined by the swap value.

This implies that the algorithm takes two inputs of the same size where the one is holding the values to be permuted, denoted $deck$ with $n$ $c_i$ values, for $i=0,\dots,n$. The other holding the values for which the different $c_i$ in the $deck$ is to be swapped, denoted the $seeds$ with $n$ $r_i$ values. If the swap values $r_i$ from the $seeds$ are not given in the correct interval the probability for the different permutations is not equally likely. Therefore it is important that the $r_i$ values are chosen accordingly to the algorithm. The algorithm states that $r_i$ is chosen from an interval starting with its own index $i$ to the size of the $deck$ which is $n$. This gives exactly the number of permutations required as the first card of $deck$ denoted $c_1$ has exactly $n$ possible places to go. $c_2$ has one less possible places to go an so forth until the algorithm reaches the last card $c_n$ which has no other place to go. Since $r_i\in[i,n]$ we have $n!$ because $i$ runs from $1$ to $n$ which should be the case as described in chapter \ref{ch:intro}.

If the values contained in $seeds$ is not chosen for the right interval but instead chosen on all $r_i$ from $0$ to $n$ we would end up having a skew on probability of the different permutations. As $r_i$ in this case has $n$ possible places to go yields $n^n$ distinct possible sequences of swaps. This introduces an error into the algorithm as there are only $n!$ and $n^n$ cannot be divisible by $n!$ for $n>2$. Resulting in a non uniform probability for the different permutations. The same is the problem if $r_i$ is not chosen from $[i,n]$ but instead $]i,n]$ such that the own index is not in the interval. By introducing this error to the algorithm the empty shuffle is not possible. In other words it is not possible to get the same output as the input. Which does not give the desired uniform distribution of permutations.

\bigskip

As described in the section on poker in chapter \ref{ch:intro} we only need a permutation of the first 20 cards. Which means that we only need the $\frac{52!}{32!}$ specific permutations out of the total of $52!$ different permutations. Doing a $m$ out of $n$ permutation using the $Fisher\text{-}Yates$ algorithm is straight forward. Instead of running through $n$ swaps indicated by the size of $seeds$ it is enough to run through $m$ swaps. Resulting in the input $seeds$ only need to have size $20$ and therefore a for-loop running fewer rounds. Those giving us a full permutation on the first $m$ indexes of $deck$.

\input{fisher_yates_fig}

%%%%%%%%%%%%%%%%%%%%% SHUFFLE NETWOKS %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Shuffle Networks}
Shuffling networks or permutation networks has a lot of resemblance to sorting networks. The idea behind this type of networks is that they consist of a number of input wires and equally many output wires. These wires go straight through the entire network. On each pair of wires there is placed a conditional swap gate. Such that if the condition of the gate is satisfied the input on the two wires are swapped. By placing these swap gates correctly on the input wires it is possible to get a complete uniform random permutation of the input on the output wires.

Applying such a shuffle network in the setting of a poker game is simple. The input to the shuffle algorithm is the $deck$ that we want to shuffle and the output is the shuffled $deck$. The more interesting part is how to place the swap gates to ensure that the right number of possible permutations is satisfied. There are many different shuffle algorithms that can be implemented using shuffle networks. The first and only I have looked into and implemeted builds on ideas from \citeA{psi} where they introduces conditional swap. The algorithm is a combination of the well known $bubble\text{-}sort$ algorithm and the conditional swap.

\bigskip

\paragraph{Conditional Swap:}
\input{con_swap}

The conditional swap algorithm takes ones again two inputs where the first input is an array, here a $deck$ of $n$ cards $c_i$ for $i=1,\dots,n$. The second input is differint from the other algorithm. This time an array $seeds$ of size $l=\frac{n^2}{2}$ bits $b_j$ where $j=1,\dots, l$. The algorithm creates $n-1$ layers of conditional swap gates where each layer is decreasing by one in size. Each layer is constructed such that each swap gate overlap with one input wire. Then each layer is made such that the first layer contains $n-1$ swap gates. The second layer $n-2$ and so on to the last layer containing only one gate. The layers are stacked in a way such that the first input wire is only represented in the first layer. This resembles what is done in the $Fisher\text{-}Yates$ algorithm, where output $c_1$ is determined by the first round in the for-loop. It can be seen in such a way that the first input $c_1$ has $n$ places to go. The second layer determines which output $c_2$ will have. Continuing this way until reaching the last layer and the two last outputs $c_{n-1}$ and $c_n$ is determined. Resulting in a shuffle algorithm with a perfect shuffle and $n!$ different permutations as wanted.

This algorithm requires much more randomness as input compared to the $Fisher\text{-}Yates$ algorithm. But this algorithm does not have the same problems of how the randomness should be chosen. This algorithm uses one bit of randomness at a time and therefore do not suffer from the problem of choosing randomness outside of the correct interval. Therefore this algorithm is more robust in terms of the input seeds. But if the inner for-loop is not running fewer and fewer rounds it will suffer the same problems as encountered by $Fisher\text{-}Yates$ because it will produce $n^n$ distinct possible sequences of swaps which is not compatible with the $n!$ possible permutations. This resulting in a skew of the probability of the different permutations such that this is no longer uniform.

\bigskip
Once again it is possible to do some optimization to the algorithm since we do not need a complete permutation of the $n$ inputs, but it is enough to only have $m$ out of $n$. This can be done as in the case for the $Fisher\text{-}Yates$ where we let the outer loop run for $m$ iterations and then we are done. This yields $n$ possible values for $c_1$, $n-1$ possible values for $c_2$ and so one until $n-m$ values for $c_{n-m}$. This is exactly the amount of permutation we require for our algorithm as this gives us $\frac{n!}{(n-m)!}$.

\input{con_swap_fig}

%%%%%%%%%%%%%%%%%%%%%%% COMPARISONS %%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Algorithm comparison}
\todo{describe circuit}
First of all it is important to understand that we cannot just plug the algorithm in to the MPC protocol. Since we use a MPC protocol that uses garbled circuits for the evaluation of the shuffle algorithm we need to provide a circuit that represent this algorithm. Therefor it is essential to understand how such circuits works and how to construct them. These circuits consist of different gates types. Special for the MPC protocol used here shuch type of circuit are all constructed of gates which has two input and one output wire. This results in 16 different gate types which can be used to construct a circuit. These 16 different gate types comes from the result of having two different values $0$ and $1$ for each of the wires. This gives $4$ different ways to combine the two values which results in $2^4$ different combinations. Of these 16 different gates are some of the well known $AND$, $OR$ and $XOR$. All the different gate types can be seen in figure.

\input{truth_table}

Since circuits is a static representation of a given algorithm it grow fast in size and become rather complex. Therefor to make the construction of the circuits for the shuffle algorithms easier I have used a compiler called $Frigate$\footnote{I used version 2 which is linked to in appendix \ref{appendix1}. This version is optimized to construct circuits for the duplo protocol.} to help with the circuit generation. The compiler takes a high-level program of a $C$ like structure and translates it to a circuit of the correct format for the MPC protocol chosen to use. This compiler gives the possibility to easy implement the shuffle algorithm and generate a complex circuit that represent the right algorithm.

\bigskip
In the next section I will try to compare the to different algorithms and their circuit representation. The first to look for in the two algorithms is the amount of loops. In the $Fisher\text{-}Yates$ algorithm there is one for-loop and which makes a direct swap resulting in $n$ total swaps. Where as in the $Conditional~swap$ algorithm we have both an outer for-loop and an inner for-loop. Which at creates $\frac{n^2}{2}$ calls to the conditional swap function. At first sight this seems to be many more calls to swap than for the $Fisher\text{-}Yates$ algorithm. But as mentioned earlier circuits are a static representation of an algorithm and therefore the two algorithms do not differ much from each other when comparing their circuit representation. First of all since circuits is a static representation of the algorithm the $Fisher\text{-}Yates$ algorithm can not do a swap based on an input to the algorithm unless all possible swaps are represented in the circuit. This resulting in the need for conditional swaps in the algorithm. Therefore in each of the rounds of the for-loop it should be possible for a value at a given index to go to all the indexes there or higher. This resulting in $n-1$ conditional swaps in the first round. $n-2$ conditional swaps in the second round and so forth. Since both algorithms now need to use conditional swaps they are easy to compare. We know that this gives exactly the same amount of conditional swaps for both algorithms, which is $(n-1)!$ if we do not take the optimizations into account. The biggest difference is how the two algorithms swap in the rounds of the outer for-loop. Here the $Fisher\text{-}Yates$ algorithm has conditional swaps from index $i$ to all the indexes $i'$ where $i<i'$. Where the $Conditional~swap$ algorithm has swaps from $j$ to $j'= j+1$ where $j$ is running from $i$ defined by the forloop to $n-1$.

Another different aspect of the algorithms is to compare the input. Both algorithms takes a $deck$ and $seeds$ as input. Where the $deck$ is the representation of the cards which is the same for both algorithms. But the input $seeds$ differs a lot. This is because of how the two algorithms handle the swaps. Therefore this is the interesting part to look at. First of all it is important to remember what the goal is for this thesis. It is to create a secure distributed poker game. Therefore we use a MPC protocol that help us overcome the security part. We can therefore expect that one of the players will try to cheat. As the protocol takes $seeds$ from both parties on how to shuffle the $deck$ we need to handle this in some way. The easy way is just to $XOR$ these $seeds$ together to get a new seed to use in the shuffle algorithm. This is completely fine in the $Conditional~swap$ algorithm since it uses one bit of randomness at a time for each swap gate. We know that the $XOR$ of to random bits yields a equally random bit. But for the $Fisher\text{-}Yates$ it makes the algorithm insecure since the $XOR$ of the two $seeds$ can result in a new seed that do not forfill the requirements to the inputs. This will give a bias that result in a higher probability of getting low cards. This is because the algorithm relies on the random $r$ values of the seed to be in given intervals. Therefore when $XOR$ the $seeds$ from the two parties it can not be guarantee that the random $r$ values for the shuffle is inside the correct interval. The easy solution to this will be to take the modulo reduction of $r$ to fit inside the desired interval. Here I will come with an example to show the problems by doing so. In our case we need to represent $52$ cards. These can be represented in base 2 using 6 bits since $\lceil{log_2(52)}\rceil=6$. But as we know $2^6=64$. This implies that we have $11$ possible values for our $r$ in the first round after the $seeds$ have been $XOR$'ed together. This result in the first $11$ values from $0$ to $10$ to have the probability $\frac{2}{64}$ while the last $31$ values from $11$ to $51$ have probability $\frac{1}{64}$ giving us a bias. 
Instead of using $XOR$ to construct the new $r$ values for the shuffle algorithm the $seeds$ will be added $6$ bits at a time. This resulting in a uniform propability on the $r$ values. Here it is important to notice that while $6$ bits is enough to hold the 52 card values it is not sufficient to hold the sum of such two values.

\bigskip

At last a short comment on other shuffle algorithmic possibilities. There do exist other types of sorting networks that could be used. In \citeA{psi} they also uses a algorithm known as the $Bitonic$ algorithm refering to the way the network is constructed. Such a network is constructed of what is known as $half-cleansers$ known from sorting networks. These $half-cleansers$ are constructed such that the input has one peak, $i_1\leq \dots \leq p \geq \dots \geq i_n$. Then the output is half sorted such that the highest values are in one of the two halfs. This creates a circuit of size $O(n\cdot log(n))$ which is better then what the $Conditional~swap$ and $Fisher\text{-}Yates$ algorithms can aquire since it creates a circuit of size $O(n^2)$. But as argued earlier the $Conditional~swap$ and $Fisher\text{-}Yates$ algorithms are easily optimized to the setting studied in this thesis.

\todo{Bitonic algorithm, optimization? can maybe be done by flipping such a sorting network and removing half cleansers}

As a result of this we get that for some card games it can be an idea to check if one algorithm can outperform another. In out case we need only $20$ out of $52$ cards. We now know that a $Bitonic$ algorithm would produce a smaller output but since the two algorithm studied here are easy to optimize such that they produce a relative small circuit. This implies that there will ba a cross over at some point where it is better to shuffle a complete deck than only parts of it even if only a part is needed. This could be in a setting when playing with more the two players for a game of poker.